

\documentclass[a4paper,11pt]{article}
\usepackage{ajmesa}
\usepackage[top = 1in, bottom = 1in, left = 0.5in, right = 0.5in]{geometry}

\pagestyle{fancy}
\fancyhf{}
%\rhead{Math 40 - WFY | Julius Basilla}
%\chead{2019-04243}
\lhead{EEE 137 Key concepts and equations}
\cfoot{Page \thepage \hspace{1pt} of \pageref{LastPage}}

\title{\textbf{EEE 137 Key concepts and Equations}}
\author{AJ Mesa Jr.}

\bibliography{137_references}

\begin{document}
	\maketitle
	\tableofcontents
	
	\section{Sets and Probabilities}
	Some set operations and properties:
	\begin{outline}[enumerate]
		\1 If $A \subseteq B$ and $B \subseteq A$, then $A = B$
		\1 The set difference $A - B$ or $A \setminus B$ contains the elements in A that are not in 
		\1 The set $A', \overline{A}, \text{or} A^C$ contains all the elements in the universal set but not in $A$ 
		\1 Commutative: 
			\2 $A \cap B = B \cap A$ 
			\2 $A \cup B = B \cup A$
		\1 Distributive: 
			\2 $A \cup \left(B \cap C\right) = \left(A \cup B\right)\cap \left(A \cup C\right)$
			\2 $A \cap \left(B \cup C\right) = \left(A \cap B\right)\cup \left(A \cap C\right)$
		\1 Associative: 
			\2 $A \cup \left(B \cup C\right) 	= \left(A \cup B\right) \cup C$
			\2 $A \cap \left(B \cap C\right) 	= \left(A \cap B\right) \cap C$
		\1 De Morgan's Law:
			\2 $\overline{A \cup B} = \overline{A} \cap \overline{B}$
			\2 $\overline{A \cap B} = \overline{A} \cup \overline{B}$	
		\1 Duality principle: the following replacements preserves the identity 
			\2 $\cup \longleftrightarrow \cap$
			\2 $S \longleftrightarrow \varnothing$	
			\2 $A \longleftrightarrow \overline{A}$
	\end{outline}
	\subsection*{Stochastic experiment}
	\begin{outline}[enumerate]
		\1 Experiment: process that gives information
		\1 Outcome: information recorded as result of experiment
		\1 Stochastic (random) experiment: process with different outcomes
		\1 Replicate: single performance of random experiment
		\1 Sample space: set which every outcome is represented by one and only one element
			\2 Discrete: countable elements
			\2 Continuous: elements are uncountable
		\1 Event: any occurence that result from performance of experiment:
			\2 Elementary: single outcome
			\2 Compound: more than 1 outcome
		\1 Probability: 
			\2 Classical: $P(E) = \frac{\text{number of favorable outcomes}}{\text{number of possible outcomes}}$
			\2 Empirical: $P(E) = \frac{\text{frequency of occurence of favorable outcome}}{\text{total frequency}}$	
		\1 Axioms of probability:
			\2 For any event $E \subseteq S,~0 \leq P(E) \leq 1$
			\2 $P(S) = 1$
			\2 $P(E \cup F) = P(E) + P(F) - P(E \cap F)$
			\2 $P(\overline{E}) = 1 - P(E)$
			\2 Principle of Inclusion and Exclusion: $P\left(\bigcup _{i=1}^{n}A_{i}\right)=\sum _{k=1}^{n}\left((-1)^{k-1}\sum _{I\subseteq \{1,\ldots ,n\} \atop |I|=k} P (A_{I})\right)$	
			\2 where $A_{I}:=\bigcap _{{i\in I}}A_{i}$
	\end{outline}

	\section{Conditional Probability}
	\begin{outline}[enumerate]
		\1 probability of event $A$ given $B$: $P\left(A | B\right) = \frac{P\left(A \cap B\right)}{P\left(B\right)}$
		\1 special cases:
		\2 $A$ is subset of $B: P\left(A | B\right) = \frac{P\left(A\right)}{P\left(B\right)}$
		\2 $B$ is subset of $A: P\left(A | B\right) = 1$
		\2 $A$ and $B$ are disjoint: $P\left(A | B\right) = 0$
		\1 Independent events: $P\left(A | B\right) = P\left(A\right)$ 
		\2 $P\left(A \cap B\right) = P\left(A\right)P\left(B\right)$
		\1 Bayes' Theorem: If a sample space $S$ is partitioned into a set of events $A_{i}$ for $i = 1,~2,\ldots,~n$ such that $\bigcup_{i=1}^{n} A_{i} = S$, then $P\left(B\right) = \sum_{i = 1}^{n}P\left(B \cap A_{i}\right) = \sum_{i = 1}^{n}P\left(B | A_{i}\right)P\left(A_{i}\right) $	
		\1 $P\left(A_{j} | B\right) = \frac{P\left(B | A_{j}\right)P\left(A_{j}\right)}{\sum_{i = 1}^{n}P\left(B | A_{i}\right)P\left(A_{i}\right)}$
		\1 Conditional expectation: $E\left[X | A\right] = \sum_{x} xp_{X | A} \left(x\right)$
	\end{outline}
	
	\section{Random Variables}
	A random variable is a function that associates a unique numerical values with every outcome of an experiment
	\begin{outline}[enumerate]
		\1 Discrete RV: has a countable number of distinct values
		\1 Continuous RV: has infinite number of possible values
		\1 Probability Distribution Function $(p_X(x))$: list of probabilities associated with each possible values of RV. 
		\1 $p(x_{i}) = P(X = x_{i})$ 
		\1 Cumulative Distribution Function $F_X(x)$: probability that a funmber is less than or equal to $x$ 
		\1 $F_X(x) = P(X \leq x)$
		\1 properties of $p_{X}(x)$:
			\2 follows that axioms of probability
			\2 $\sum_{i} P_X(x_{i}) = 1$
		\1 properties of $F_{X}(x)$:
			\2 if $x_{1} < x_{2}$, then $F_{X}(x_{1}) \leq F_{X}(x_{2})$
			\2 $F_{X}(+\infty) = 1,~ F_{X}(-\infty) = 0$
			\2 $P(X > x) = 1 - F_{X}(x)$
	\end{outline}

	\subsection{Discrete Random Variables}
	\begin{outline}[enumerate]
		\1 for disjoint events $A$ that occurs $m$ ways and $B$ that occurs $n$ ways, the event $A$ or $B$ occurs in $m + n$ ways 
		\1 suppose events $C$ can be decomposed of steps $A$ and $B$ (unrelated events). $C$ occurs $mn$ ways
		\1 Bernoulli trial: only 2 possible outcomes. has probability mass function 
		\begin{equation}
			P_{X}(k) = \binom{n}{k}p^{k}\left(1 - p\right)^{n - k}
		\end{equation}
		\1 Expectation: $E\left[X\right] = \overline{x} = \frac{\sum_{i} N_{i}x_{i}}{\sum_{i} N_{i}} = \sum_{i} x_{i}p_{X}(x_{i})$
		\1 Properties of expectation:
			\2 $E[c] = c$
			\2 $E[cg(x)] = cE[g(x)]$
			\2 $E[g_{1}(x) + g_{2}(x) = E[g_{1}(x)] + E[g_{2}(x)]$
		\1 Variance: $\sigma_{X}^2 = E\left[\left(X - \overline{X}\right)^{2}\right] = \overline{\left(X - \overline{X}\right)^{2}} = E\left[X^{2}\right] - \left(E\left[X\right]\right)^{2}$ 	
		\1 Standard deviation: $\sigma_{X}$
		\1 Mean square value: $E[X^{2}] = \overline{X^{2}}$
		\1 $n$th moment: $m_{n} = E\left[X^{n}\right] = \overline{X^{n}}$
		\1 $n$th central moment: $\mu_{n} = E\left[\left(X - \overline{X}\right)^{n}\right] = \overline{\left(X - \overline{X}\right)^{n}}$	
		\1 for Bernoullie trial:
			\2 $E\left[X\right] = 0(1 - p) + 1(p) = p$
			\2 $E\left[X^{2}\right] = 0^{2}(1 - p) + 1^{2}p = p$
			\2 $\sigma_{X}^{2} = p - p^{2}$
			\2 number of successes on $n$ trials: $p_{X}(x) = \binom{n}{x}p^{x}\left(1 - p\right)^{n - x}$
			\2 mean: $E\left[X\right] = (np)\sum\limits_{y = 0}^{n - 1} \binom{n - 1}{y}p^y\left(1 - p\right)^{n - \left(y + 1\right)} = np$
			\2 mean square: $E\left[X^{2}\right] = np\left(1 - p\right) + n^{2}p^{2}$
			\2 variance: $\sigma_{X}^{2} = np\left(1 - p\right)$
	\end{outline}

	\subsection{Continuous Random Variables}
	\begin{outline}[enumerate]
		\1 Expectation: $E\left[X\right] = \overline{x} = \int\limits_{-\infty}^{+\infty} xf_{X}\left(x\right) \dd x$
		\1 Uniform Random Variable: sub-intervals of the same length are equally likely
		\1 Gaussian: model for natural random phenomena:
			\2 Probability Distribution Function: 
			\begin{equation}
				f_{X}(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{\left(x - \overline{x}\right)^{2}}{2\sigma^2}\right)
			\end{equation} 
			\2 mean: $\overline{x}$
			\2 variance: $\sigma^{2}$
		\1 any Gaussian RV has the standard form: $f_{X}(x) = G\left(\frac{x - \overline{x}}{\sigma}\right)$
		\1 Exponential Random Variable : model for waiting time
		 	\2 Probability Distribution Function: 
		 	\begin{equation}
		 		f_{X}(t) = \lambda\exp\left(-\lambda t\right);~\lambda \geq 0
		 	\end{equation}
		 	\2 mean: $\frac{1}{\lambda}$
		 	\2 variancec: $\frac{1}{\lambda^{2}}$	
	\end{outline}

	\subsection{Bivariate Random Variable}
	\begin{outline}[enumerate]
		\1 Joint distribution: determined by 2 independent random variables
			\2 $F_{X, Y} = \sum_{n = 1}^{N}\sum_{m = 1}^{M}P\left(x_{n}, y_{m}\right)u\left(x - x_{n}\right)u\left(y - y_{m}\right)$
			\2 $P\left(x_{n}, y_{m}\right)$ is the probability of the join event $\{X = x_{n}, Y = y_{m}\}$
		\1 Properties of Joint Distribution:
			\2 $F_{X, Y}\left(-\infty, -\infty\right) = F_{X, Y}\left(x, -\infty\right) = F_{X, Y}\left(-\infty, y\right) = 0$
			\2 $F_{X, Y}\left(+\infty, +\infty\right) = 1$
			\2 $0 \leq F_{X, Y}\left(x, y\right) \leq 1$
			\2 $P\{x_{1} < X \leq x_{2}. y_{1} < Y \leq y_{2}\} = F_{X, Y}\left(x_{2}, y_{2}\right) + F_{X, Y}\left(x_{1}, y_{1}\right) - F_{X, Y}\left(x_{1}, y_{2}\right) - F_{X, Y}\left(x_{2}, y_{1}\right)$	
			\2 Marginal Distribution Functions:
				\3 $F_{X, Y}\left(x, +\infty\right) = F_{X}(y)$
				\3 $F_{X, Y}\left(+\infty, y\right) = F_{y}(y)$	
		\1 Joint Probability Density Function
			\begin{align}
				f_{X, Y}\left(x, y\right) &= \diffp{F_{X, Y}\left(x, y\right)}{{x}{y}} \\
				f_{X, Y}\left(x, y\right) &= \sum_{n = 1}^{N}\sum_{m = 1}^{M}P\left(x_{n}, y_{m}\right)\delta \left(x - x_{n}\right)\delta \left(y - y_{m}\right)
			\end{align}		
		\1 Properties of Joint PDF:
			\2 $f_{X, Y}\left(x, y\right) \geq 0$
			\2 $\int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} f_{X, Y}\left(x, y\right) \dd x \dd y = 1$	
			\2 $F_{X, Y}\left(x, y\right) \int\limits_{-\infty}^{y} \int\limits_{-\infty}^{x} f_{X, Y}\left(x', y'\right) \dd x' \dd y'$
			\2 $F_{X}\left(x\right) \int\limits_{-\infty}^{x} \int\limits_{-\infty}^{+\infty} f_{X, Y}\left(x', y'\right) \dd y' \dd x'$
			\2 $F_{Y}\left(y\right) \int\limits_{-\infty}^{y} \int\limits_{-\infty}^{+\infty} f_{X, Y}\left(x', y'\right) \dd x' \dd y'$
			\2 $P\left(x_{1} < X \leq x_{2}, y_{1} < Y\leq y_{2}\right) = \int\limits_{y_{1}}^{y_{2}} \int\limits_{x_{1}}^{x_{2}} f_{X, Y}\left(x', y'\right) \dd x' \dd y'$
			\2 marginal distribution of $X: f_{X}\left(x\right) = \int\limits_{-\infty}^{+\infty} f_{X, Y}\left(x', y'\right) \dd y' = \diff{F_{X}\left(x\right)}{x}$
			\2 marginal distribution of $Y: f_{Y}\left(y\right) = \int\limits_{-\infty}^{+\infty} f_{X, Y}\left(x', y'\right) \dd x' = \diff{F_{Y}\left(y\right)}{y}$
			\2 $X$ and $Y$ are statistically independent if $f_{X, Y}\left(x, y\right) = f_{X}\left(x\right)f_{Y}\left(y\right)$
		\1 Conditional distribution:
			\2 $f_{X, Y}\left(y | x\right) = \frac{f_{X, Y}\left(x, y\right)}{f_{X}\left(x\right)}$
			\2 $f_{X, Y}\left(x | y\right) = \frac{f_{X, Y}\left(x, y\right)}{f_{Y}\left(y\right)}$	
		\1 Independence: Two events $\{X = x\}$ and $A$ are independent if $\bvec{P}\left(X = x \text{ and } A\right) = \bvec{P}\left(X = x\right)\bvec{P}\left(A\right) = p_{X}\left(x\right)\bvec{P}\left(A\right)$	
			\2 if $P\left(A\right) > 0$, then $p_{X | A}\left(x\right) = p_{X}\left(x\right)$
		\1 Joint moment about the origin:
			\begin{equation}
				m_{n, k} = E\left[X^{n} Y^{k}\right] = \iint
				\limits_{-\infty}^{+\infty} x^{n} y^{k} f_{X, Y} \left(x, y\right) \dd x \dd y
			\end{equation}	
		\1 Correlation: $R_{XY} = m_{1, 1} = E\left[XY\right]$	
			\2 if $X$ and $Y$ are statistically independent then $R_{X, Y} = E\left[X\right]E\left[Y\right]$ and $X$ and $Y$ are \textit{uncorrelated}
			\2 if $R_{XY} = 0$, then $X$ and $Y$ are \textit{orthogonal}
		\1 Joint Central moments: $\mu_{n, k} = E\left[\left(X - \overline{X}\right)^{n} \left(Y - \overline{Y}\right)^{k} \right]$	
			\2 Covariance: $C_{XY} = \mu_{1, 1} = E\left[\left(X - \overline{X}\right) \left(Y - \overline{Y}\right) \right] = R_{XY} - E\left[X\right]E\left[Y\right]$
			\2 Normalized covariance: $\rho_{XY} = \frac{C_{XY}}{\sigma_{X}\sigma_{Y}} = \frac{R_{XY} - E\left[X\right]E\left[Y\right]}{\sigma_{X}\sigma_{Y}}$
		
	\end{outline}

	\section{Functions of Random Variables}
	\subsection{Transformation of RV}
	\begin{outline}[enumerate]
		\1 Suppose $X$ is a discrete RV with probability distribution $f\left(x\right)$. Let $Y = g\left(X\right)$ be a one-to-one transformation such that $x = h\left(y\right)$. The probability distribution of $Y$ is
			\begin{equation}
				f_{Y}\left(y\right) = f\left[h\left(y\right)\right]		
			\end{equation}
		\1 the process for joint probability is the same (but you hve two substitutions)
		\1 Suppose $X$ is a continuous RV with probability distribution $f_{X}\left(x\right)$. Let $Y = g\left(X\right)$ be a one-to-one transformation such that $x = h\left(y\right)$. The probability distribution of $Y$ is 
			\begin{equation}
				f_{Y}\left(y\right) = f\left[h\left(y\right)\right]\lvert J\rvert	
			\end{equation}
		where $J = \diff{h}{y}$	and is called the Jacobian of the transformation
		\1 for joint probability, the Jacobian is $J = \det\begin{bmatrix}
		\diffp{x_{1}}{{y_{1}}} & \diffp{x_{1}}{{y_{2}}} \\ 
		\diffp{x_{2}}{{y_{1}}} & \diffp{x_{2}}{{y_{2}}}
		\end{bmatrix}$
	\end{outline}
	\subsection{Linear Combination of RV}
	\begin{outline}[enumerate]
		\1 Suppose $Z = X + Y$ where $X$ and $Y$ are independent
			\2 Discrete case:
				\3 $P\left(Z = z\right) = \sum\limits_{k = -\infty}^{+\infty} P\left(X = k\right) P\left(Y = z - k\right)$
			\2 Continuous case:	
				\3 cumulative function is $F_{Z}\left(z\right) = P\left(Z \leq z\right) = P\left(X + Y \leq z\right) = \int\limits_{R} f_{X, Y}\left(x, y\right) \dd x \dd y$
				\3 probability distribution function: $f_{Z}\left(z\right) = \int\limits_{-\infty}^{+\infty} f_{Y}\left(z - x\right)f_{X}\left(x\right) \dd x \equiv f_{Y}\left(y\right) * f_{X}\left(x\right)$
	\end{outline}

	\section{Central Limit Theorem}
	\begin{outline}[enumerate]
		\1 Population: totality of concerned observations
		\1 Sample: subset of the population
			\2 Sample mean: $\overline{X} = \frac{1}{n}\sum_{i = 1}^{n} X_{i}$	
			\2 Sample median: $\tilde{x} = \left\{\begin{array}{ll}
				x_{\left(n + 1\right)/2}, & \text{if $n$ is odd} \\
				\frac{1}{2}\left(x_{n/2} + x_{\left(n + 2\right)/2}\right), & \text{if $n$ is even}
			\end{array} \right.$ 
			\2 Sample mode: most frequent
			\2 Sample variance: $S^2 = \frac{1}{n - 1}\sum_{i = 1}^{n} \left(X_{i} - \overline{X}\right)^{2} = \frac{1}{n\left(n - 1\right)} \left[n\sum_{i = 1}^{n}X_{i}^{2} - \left(\sum_{i = 1}^{n} X_{i}\right)^2\right]$
			\2 Sample standard deviation: $S = \sqrt{S}$
			\2 Sample range: $R = X_{\text{max}} - X_{\text{min}}$
		\1 Central Limit Theorem: If $\overline{X}$ is the mean of a random sample of size $n$ taked from a population with mean $\mu$ and finite variance $\sigma^{2}$, then the limiting form of the distribution of 
		\begin{equation}
			Z = \frac{\overline{X} - \mu}{\sigma / \sqrt{n}}
		\end{equation}	
		as $n \to \infty$ is the standard normal distribution $n\left(z; 0, 1\right)$
		\1 Consequences:
			\2 let $\overline{X} = \frac{1}{n}\sum_{i = 1}^{x} X_{i}$ and $S_{n} = \sum_{i = 1}^{x} X_{i}$
			\2 $\overline{X}$ and $S_{n}$ have approximately normal distribution
			\2 $\sigma_{\overline{X}}^{2} = \sigma^{2}/n$
			\2 $\mu_{\overline{X}} = \mu$
			\2 $\mu_{s} = n\mu$
			\2 $\sigma_{S}^{2} = n\sigma^2$
		\1 for two independent samples of size $n_{1}$ and $n_{2}$:
		\begin{equation}
			Z = \frac{\left(\overline{X}_{1} - \overline{X}_{2}\right) - \left(\mu_{1} - \mu_{2}\right)}{\sqrt{\left(\sigma_{1}^{2} / n_{1}\right)^{2} + \left(\sigma_{2}^{2} / n_{2}\right)^{2}}}
		\end{equation}	
	\end{outline}
	
\end{document}
